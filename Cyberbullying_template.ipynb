{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the AIinAfrica Hackaton! \n",
    "## Cyberbullying Contest\n",
    "### Introduction\n",
    "\n",
    "We present an analysis of emotions linked to tweets in order to detect instances of cyberbulling.\n",
    "The tweets dataset has been manually collected using twitter APIs by Margarita Bugueño, Fabián Fernandez and Francisco Mena.\n",
    "\n",
    "The NRC Emotion Lexicon (aka Emolex) is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). It has been developed by [Saif Mohammad](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) and is a lexic tag based on the Plutchick wheel of emotions. The annotations were manually done by crowdsourcing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "To begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 44 csv files in the current version of the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "print(os.listdir('data'))\n",
    "print(os.listdir('data/tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing one file alyssalg93.csv \n",
    "\n",
    "tweets = pd.read_csv('data/tweets/alyssalg93.csv', delimiter=',')\n",
    "tweets.dataframeName = 'alyssalg93.csv'\n",
    "print(f'There are {tweets.shape[0]} rows and {tweets.shape[1]} columns')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the NRC Emoticon Lexicon\n",
    "We will be using the NRC Emotion Lexicon for the sentiment analysis of the tweets.\n",
    "The NRC Emotion Lexicon is a list of English words and their associations with eight basic e motions \n",
    "(anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments \n",
    "(negative and positive). The annotations were manually done by crowdsourcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "lexicon = \"NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "emolex_df = pd.read_csv(lexicon,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep='\\t')\n",
    "\n",
    "emolex_df.dropna(subset=['word'], inplace=True)\n",
    "\n",
    "emolex_words = emolex_df.pivot(index='word',\n",
    "                                columns='emotion',\n",
    "                                values='association')\n",
    "\n",
    "emolex_words.index = emolex_words.index.map(lambda w: stemmer.stem(w.lower()) if w else nan)\n",
    "\n",
    "emotions = emolex_words.columns.values\n",
    "\n",
    "emolex_words['emotions'] = list(zip(emolex_words.anger, emolex_words.anticipation, emolex_words.disgust,\n",
    "                                  emolex_words.fear, emolex_words.joy, emolex_words.negative, emolex_words.positive,\n",
    "                                   emolex_words.sadness, emolex_words.surprise, emolex_words.trust))\n",
    "\n",
    "# Convert into a dictionary for faster lookup\n",
    "emolex_dict = emolex_words['emotions'].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "print(\"We built a dictionary of {} words associated to emotions\".format(len(emolex_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once to import punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to score the tweets\n",
    "def text_emotion(df, column):\n",
    "    '''\n",
    "    Takes a DataFrame and a specified column of text and adds 10 columns\n",
    "    for each of the 10 emotions in the NRC Emotion Lexicon, with each\n",
    "    column containing the value of the text in that emotions and the counts of tweets\n",
    "    INPUT: DataFrame, string\n",
    "    OUTPUT: New DataFrame with ten new columns\n",
    "    '''\n",
    "\n",
    "    new_df = df.drop(['id', 'favorite count', 'retweet count', 'created at'], axis=1)\n",
    "    new_df['document'] = pd.Series()\n",
    "    new_df = new_df.reindex(columns=new_df.columns.tolist())\n",
    "    # Convert to numpy array\n",
    "    tweets = new_df.copy().to_numpy()\n",
    "    scores = np.zeros((tweets.shape[0], len(emotions)))\n",
    "    #print(scores.shape)\n",
    "\n",
    "\n",
    "    with tqdm(total=new_df.shape[0]) as pbar:\n",
    "        for i, text in enumerate(tweets[:, 0]):        \n",
    "            pbar.update(1)\n",
    "            #print(\"Iteration \",i)\n",
    "            document = word_tokenize(text)\n",
    "            tweets[i, 2] = document\n",
    "            for w, word in enumerate(document):\n",
    "                document[w] = stemmer.stem(word.lower())\n",
    "                #emo_score = emolex_words[emolex_words.word == word].values\n",
    "                emo_score = emolex_dict.get(word)\n",
    "                if emo_score != None:\n",
    "                    scores[i,:] += list(emo_score)\n",
    "    \n",
    "    tweets_df = pd.DataFrame(data=tweets, columns=new_df.columns)\n",
    "    scores_df = pd.DataFrame(data=scores, columns=emotions)\n",
    "\n",
    "    return pd.concat([tweets_df, scores_df], axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read all the files and build one dataframe with the emolex scores from all the tweets in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read all set of tweets and build sentiment dataframes\n",
    "df_emo_all = pd.DataFrame()\n",
    "for file in os.listdir('data/tweets'):  \n",
    "    df = pd.read_csv('data/tweets/'+file, delimiter=',')\n",
    "    df['screen_name'] = os.path.splitext(file)[0]\n",
    "    print(\"Scoring tweets from \", os.path.splitext(file)[0])\n",
    "    df_emo = text_emotion(df, 'text')\n",
    "    df_emo_all = pd.concat([df_emo_all, df_emo])\n",
    "    \n",
    "df_emo_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emo_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating Grouping data together\n",
    "df_emotions = df_emo_all.groupby('screen_name')['anger','anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust','negative', 'positive'].mean()\n",
    "df_emotions['n_tweets']=df_emo_all.screen_name.value_counts()\n",
    "df_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Distribution graphs (histogram/bar graph) of column data\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    #filename = df.dataframeName\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    #plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.title(f'Correlation Matrix', fontsize=15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Scatter and density plots\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to read in the data and use the plotting functions to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution graphs (histogram/bar graph) of sampled columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "plotPerColumnDistribution(df_emotions.drop(['n_tweets'], axis=1), 4, 4)\n",
    "#plotPerColumnDistribution(df1, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(df_emotions.drop(['n_tweets'], axis=1), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter and density plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "plotScatterMatrix(df_emotions.drop(['n_tweets'], axis=1), 18, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many bullies do we have??\n",
    "\n",
    "df_emotions[df_emotions.anger > 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and potentially bullied?\n",
    "df_emotions[df_emotions.fear > 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 angriest\n",
    "df_emotions.sort_values(by='anger', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 saddest\n",
    "df_emotions.sort_values(by='sadness', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "So where from now? Can you build a predictive model based on tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play with isiZulu\n",
    "\n",
    "def build_emolex(language):\n",
    "        \n",
    "    lexicon = \"NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-v0.92_\"+language+\".txt\"\n",
    "    emolex_words = pd.read_csv(lexicon,\n",
    "                            index_col=0,\n",
    "                            sep='\\t', na_values='NO TRANSLATION')\n",
    "    print(emolex_words.shape)\n",
    "    #emolex_words.index = emolex_words.index.map(lambda w: stemmer.stem(w.lower()) if w else nan)\n",
    "\n",
    "    emotions = emolex_words.columns.values\n",
    "    emolex_words['emotions'] = list(zip(*map(emolex_words.get, emolex_words)))\n",
    "    # Convert into a dictionary for faster lookup\n",
    "    emolex_dict = emolex_words['emotions'].to_dict()\n",
    "    print(\"We built a dictionary of {} words associated to {} emotions\".format(len(emolex_dict), len(emotions)))\n",
    "    return (emolex_dict, emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language='isizulu'\n",
    "isizulu = build_emolex('isizulu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(isizulu[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to score a sentence\n",
    "from nltk import word_tokenize\n",
    "def sentence_emotions(sentence, emo_dict):\n",
    "    \"\"\"\n",
    "    Scores each word in a sentence and prints total emotional scores\n",
    "    \"\"\"\n",
    "    sentence_score = np.zeros(10)\n",
    "    emolex, emotions = emo_dict\n",
    "    document = word_tokenize(sentence)\n",
    "    for w, word in enumerate(document):\n",
    "                #document[w] = stemmer.stem(word.lower())\n",
    "                #emo_score = emolex_words[emolex_words.word == word].values\n",
    "                emo_score = emolex.get(word)\n",
    "                if emo_score != None:\n",
    "                    sentence_score += emo_score\n",
    "    print(\"Sentence scores:\")\n",
    "    for i in range(len(emotions)):\n",
    "        print(\"{}: {}\".format(emotions[i],sentence_score[i]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_emotions(\"bulala ukudabuka\", isizulu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language='isizulu'\n",
    "isizulu = build_emolex('isizulu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language='xhosa'\n",
    "sesotho, emotions = build_emolex(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language='sesotho'\n",
    "sesotho, emotions = build_emolex(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesotho.get('ho etsa lichelete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language='sesotho'\n",
    "lexicon = \"NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-v0.92_\"+language+\".txt\"\n",
    "emolex_words = pd.read_csv(lexicon,\n",
    "                            index_col=0,\n",
    "                            sep='\\t')\n",
    "\n",
    "emolex_words = emolex_words[emolex_words.index!='NO TRANSLATION']\n",
    "\n",
    "print(emolex_words.shape)\n",
    "#emolex_words.index = emolex_words.index.map(lambda w: stemmer.stem(w.lower()) if w else nan)\n",
    "\n",
    "emotions = emolex_words.columns.values\n",
    "emolex_words['emotions'] = list(zip(*map(emolex_words.get, emolex_words)))\n",
    "emolex_dict = emolex_words['emotions'].to_dict()\n",
    "print(\"We built a dictionary of {} words associated to {} emotions\".format(len(emolex_dict), len(emotions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_set = set(emolex_words['emotions'].to_dict().keys())\n",
    "emo_set = set(emolex_words.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could develop a metric to define a \"bully\" with some tresholds\n",
    "\n",
    "def is_a_bully(screen_name, df):\n",
    "    \"\"\"\n",
    "    Test function to flag a user as a potential bully using the aggregated metrics\n",
    "    \n",
    "    \"\"\"\n",
    "    ### Your code here\n",
    "    \n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you write a scoring function that uses  bi-grams (a sequence of two words) and tri-grams (a sequence of three words)\n",
    "# instead than scoring word by word?\n",
    "# HINT: Google search for \"generate bigrams nltk\"\n",
    "\n",
    "def bigram_sentence_emotions(sentence, emo_dict):\n",
    "    \"\"\"\n",
    "    Scores single words and bigrams (sequences of two words) sentence and prints total emotional scores\n",
    "    \n",
    "    \"\"\"\n",
    "    sentence_score = np.zeros(10)\n",
    "    emolex, emotions = emo_dict\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "\n",
    "    print(\"Sentence scores:\")\n",
    "    for i in range(len(emotions)):\n",
    "        print(\"{}: {}\".format(emotions[i],sentence_score[i]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary that works for \"Joburg Zulu\" or Scamto urban language by mixing english and zulu words\n",
    "# and adding slang\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
